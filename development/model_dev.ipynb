{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development \n",
    "This notebook was used to develop the model.py, image_collator.py, get_batch.py files.\n",
    "\n",
    "It also showcases the data processing pipline and model fucntionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.PngImagePlugin.PngImageFile image mode=L size=1354x530>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=1829x730>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=L size=2110x834>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch, torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image_dir = r'C:\\Users\\hunte\\OneDrive\\Documents\\Coding Projects\\Signature-Similarity-Checker\\data\\handwritten-signatures\\sample_Signature\\sample_Signature\\forged'\n",
    "\n",
    "# Get all file names in the directory\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith('.PNG')]\n",
    "\n",
    "# Load images into a list using PIL\n",
    "images = [Image.open(os.path.join(image_dir, img_file)) for img_file in image_files]\n",
    "\n",
    "#shwo first three images\n",
    "images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shapes of images post converting to tensor: \n",
      "\n",
      "torch.Size([1, 530, 1354]) torch.Size([1, 730, 1829]) torch.Size([1, 834, 2110]) torch.Size([1, 855, 1756]) torch.Size([1, 559, 1213]) \n",
      "torch.Size([1, 736, 933]) torch.Size([1, 423, 1304]) torch.Size([1, 599, 1675]) torch.Size([1, 826, 2073]) torch.Size([1, 653, 1545]) \n",
      "torch.Size([1, 623, 1078]) torch.Size([1, 573, 1203]) torch.Size([1, 470, 1345]) torch.Size([1, 698, 1928]) torch.Size([1, 611, 1864]) \n",
      "torch.Size([1, 619, 1435]) torch.Size([1, 398, 1777]) torch.Size([1, 647, 1081]) torch.Size([1, 505, 1527]) torch.Size([1, 653, 1816]) \n",
      "torch.Size([1, 730, 2178]) torch.Size([1, 800, 1648]) torch.Size([1, 542, 1419]) torch.Size([1, 684, 1019]) torch.Size([1, 579, 1533]) \n",
      "torch.Size([1, 864, 1635]) torch.Size([1, 800, 1952]) torch.Size([1, 766, 1760]) torch.Size([1, 476, 1045]) torch.Size([1, 740, 1183]) \n",
      "\n",
      "\n",
      "Shapes of images post max pooling: \n",
      "\n",
      "torch.Size([1, 66, 169]) torch.Size([1, 91, 228]) torch.Size([1, 104, 263]) torch.Size([1, 106, 219]) torch.Size([1, 69, 151]) \n",
      "torch.Size([1, 92, 116]) torch.Size([1, 52, 163]) torch.Size([1, 74, 209]) torch.Size([1, 103, 259]) torch.Size([1, 81, 193]) \n",
      "torch.Size([1, 77, 134]) torch.Size([1, 71, 150]) torch.Size([1, 58, 168]) torch.Size([1, 87, 241]) torch.Size([1, 76, 233]) \n",
      "torch.Size([1, 77, 179]) torch.Size([1, 49, 222]) torch.Size([1, 80, 135]) torch.Size([1, 63, 190]) torch.Size([1, 81, 227]) \n",
      "torch.Size([1, 91, 272]) torch.Size([1, 100, 206]) torch.Size([1, 67, 177]) torch.Size([1, 85, 127]) torch.Size([1, 72, 191]) \n",
      "torch.Size([1, 108, 204]) torch.Size([1, 100, 244]) torch.Size([1, 95, 220]) torch.Size([1, 59, 130]) torch.Size([1, 92, 147]) \n",
      "\n",
      "\n",
      "Shapes of images post resizing: \n",
      "\n",
      "torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) \n",
      "torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) \n",
      "torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) \n",
      "torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) \n",
      "torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) \n",
      "torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) torch.Size([1, 50, 150]) \n",
      "\n",
      "\n",
      "Shapes of images post stacking: \n",
      "\n",
      "torch.Size([30, 1, 50, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hunte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from image_collator import ImageCollator\n",
    "\n",
    "collator = ImageCollator()\n",
    "\n",
    "tensor_stack = collator.collate(images, num_poolings=3, print_shapes=True, resize_size=(50, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Batches of Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_batch import Build_Batch\n",
    "\n",
    "batch_builder = Build_Batch()\n",
    "\n",
    "batch = batch_builder.build_batch(tensor_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intepretation of the below tensor size is that their are [(870 pairs of images), (2 images per pair), (1 singleton dimmension), (height of 50), (width of 150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([870, 2, 1, 50, 150])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiEncoder(\n",
       "  (conv_layer_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (linear_layer): Linear(in_features=3456, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import BiEncoder\n",
    "\n",
    "model = BiEncoder(threshold=0.5)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds shape: torch.Size([870])\n",
      "first 5 preds: tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "preds = model(batch)\n",
    "\n",
    "print(f'preds shape: {preds.shape}')\n",
    "print(f'first 5 preds: {preds[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model outputs a vector of binary preds on the similarity of the image pairs. \n",
    "\n",
    "It is not trained yet, so all the preds are ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
